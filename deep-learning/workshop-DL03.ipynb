{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop DL03: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Exercises\n",
    "* **Exercise 1**. Find your own text corpus to build an RNN on.  \n",
    "* **Exercise 2**. Implement your own RNN using your text corpus as training data. Feel free to use the `chunkify/encode` functions provided below to help with preprocessing. If you get stuck on anything, then follow the MNIST worked example provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "* http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a given passage of text into chunks of the specified size\n",
    "def chunkify(text, chunk_size):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a string into a list of integers\n",
    "def encode_string(string):\n",
    "    encoding = []\n",
    "    for char in string:\n",
    "        encoding.append([float(ord(char))])\n",
    "    return encoding\n",
    "\n",
    "# converts a list of strings into a list of lists of integers\n",
    "def encode_text(text):\n",
    "    encodings = []\n",
    "    for string in text:\n",
    "        encodings.append(encode_string(string))\n",
    "    while len(encodings[-1]) < len(encodings[0]):\n",
    "        encodings[-1].append([float(ord(\" \"))])\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, sequence_length):\n",
    "    train_X = encode_text(chunkify(text, sequence_length))\n",
    "    train_y = [train_X[i+1][0] for i in range(0, len(train_X)-1)]\n",
    "    train_y.append(float(ord(\" \")))\n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: MNIST RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "\n",
    "N_SEQUENCES = 28\n",
    "SEQUENCE_SIZE = 28\n",
    "RNN_SIZE = 128\n",
    "\n",
    "N_TRAINING_EPOCHS = 1\n",
    "TRAINING_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"input\") as scope:\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    X_box = tf.reshape(X, [-1, N_SEQUENCES, SEQUENCE_SIZE])\n",
    "    X_transpose = tf.transpose(X_box, [1,0,2])\n",
    "    X_reshape = tf.reshape(X_transpose, [-1, SEQUENCE_SIZE])\n",
    "    X_split = tf.split(X_reshape, N_SEQUENCES, 0)\n",
    "    \n",
    "with tf.variable_scope(\"rnn\") as scope:\n",
    "    lstm_cell = rnn.BasicLSTMCell(RNN_SIZE)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, X_split, dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope(\"output\") as scope:\n",
    "    W = tf.get_variable(\"weights\", shape=[RNN_SIZE, N_CLASSES])\n",
    "    b = tf.get_variable(\"biases\", shape=[N_CLASSES])\n",
    "    y_ = tf.matmul(outputs[-1], W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\") as scope:\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y, logits=y_))\n",
    "\n",
    "with tf.variable_scope(\"optimizer\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed.\n",
      "Loss: 234.14081805944443\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(N_TRAINING_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/TRAINING_BATCH_SIZE)):\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(TRAINING_BATCH_SIZE)\n",
    "            \n",
    "            _, c = sess.run([optimizer, loss], feed_dict={\n",
    "                X: epoch_x,\n",
    "                y: epoch_y\n",
    "            })\n",
    "            epoch_loss += c\n",
    "            \n",
    "        print(\"Epoch \" + str(epoch) + \" completed.\")\n",
    "        print(\"Loss: \" + str(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore This - Worked Example: Character Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_LEN = 24\n",
    "SEQUENCE_LEN = 8\n",
    "ELEMENT_LEN = 1\n",
    "N_CLASSES = 1\n",
    "N_TRAINING_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess(text, SEQUENCE_LEN)\n",
    "batches_X, batches_y = chunkify(train_X, BATCH_LEN), chunkify(train_y, BATCH_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"input\"):\n",
    "    X = tf.placeholder(tf.float32,\n",
    "        shape=[BATCH_LEN, SEQUENCE_LEN, ELEMENT_LEN])\n",
    "    X_reshape = tf.reshape(X, [-1, SEQUENCE_LEN])\n",
    "    X_split = tf.split(X_reshape, SEQUENCE_LEN, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    lstm_cell = rnn.MultiRNNCell(\n",
    "        [rnn.BasicLSTMCell(128),\n",
    "        rnn.BasicLSTMCell(128),\n",
    "        rnn.BasicLSTMCell(128)]\n",
    "    )\n",
    "    output, state = tf.nn.static_rnn(\n",
    "        lstm_cell, X_split, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"output\"):\n",
    "    W = tf.get_variable(\"weight\", shape=[128, 1])\n",
    "    b = tf.get_variable(\"biases\", shape=[1])\n",
    "    y_ = tf.matmul(output[-1], W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\"):\n",
    "    y = tf.placeholder(tf.float32, shape=[BATCH_LEN, N_CLASSES])\n",
    "    msqerr = (y - y_)**2\n",
    "    loss = tf.reduce_mean(msqerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1, N_TRAINING_EPOCHS+1):\n",
    "        for i in range(0, len(batches_X)-1):\n",
    "            sess.run([optimizer], feed_dict={X:batches_X[i], y:batches_y[i]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
