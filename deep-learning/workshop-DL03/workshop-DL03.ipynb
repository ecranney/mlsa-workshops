{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop DL03: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "* http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py-L20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Exercises\n",
    "\n",
    "1. Work through the MNIST RNN example; make sure that you understand the meaning of **SEQUENCE_LEN**, **ELEMENT_LEN**, and **N_TRAINING_BATCHES**. Also pay very close attention to the **reshape** and **transpose** steps in the first `variable_scope` block.\n",
    "2. Find your own sequential data (e.g. a text corpus) (or just use \"the-last-question.txt\"); try to recreate Karparthy's min-char-rnn for the text in tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: MNIST RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 28\n",
    "ELEMENT_LEN = 28\n",
    "\n",
    "N_CLASSES = 10\n",
    "\n",
    "N_TRAINING_EPOCHS = 10\n",
    "TRAINING_BATCH_SIZE = 128\n",
    "\n",
    "N_LSTM_CELLS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"input\") as scope:\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    X_reshape = tf.reshape(X, [-1, SEQUENCE_LEN, ELEMENT_LEN])\n",
    "    \n",
    "    # make the primary axis BATCH_LEN\n",
    "    X_transpose = tf.transpose(X_reshape, [1,0,2])\n",
    "    \n",
    "    # split the (SEQUENCE_LEN, BATCH_LEN, ELEMENT_LEN) tensor into\n",
    "    # a list of (BATCH_LEN, ELEMENT_LEN) tensors\n",
    "    X_reshape_v2 = tf.reshape(X_transpose, [-1, ELEMENT_LEN])\n",
    "    X_split = tf.split(X_reshape_v2, SEQUENCE_LEN, 0)\n",
    "    \n",
    "with tf.variable_scope(\"rnn\") as scope:\n",
    "    lstm_cell = rnn.BasicLSTMCell(N_LSTM_CELLS)\n",
    "    outputs, states = rnn.static_rnn(\n",
    "        lstm_cell, X_split, dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope(\"output\") as scope:\n",
    "    W = tf.get_variable(\"weights\", shape=[N_LSTM_CELLS, N_CLASSES])\n",
    "    b = tf.get_variable(\"biases\", shape=[N_CLASSES])\n",
    "    # outputs is shape SEQUENCE_LEN x (BATCH_LEN, ELEMENT_LEN)\n",
    "    # outputs[0] is the embedded prediction that the LSTM comes up with\n",
    "    # after having seen only the 0th element in the sequence, outputs[-1]\n",
    "    # is the embedded prediction the LSTM comes up with after having\n",
    "    # seen **all** elements in the sequence - this is the one we want\n",
    "    # to use for our final prediction\n",
    "    y_ = tf.matmul(outputs[-1], W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(150)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\") as scope:\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y, logits=y_))\n",
    "\n",
    "with tf.variable_scope(\"optimizer\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Epoch 0 completed.\n",
      "Loss: 633.796434879303\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 1 completed.\n",
      "Loss: 534.4614440202713\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 2 completed.\n",
      "Loss: 511.82694256305695\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 3 completed.\n",
      "Loss: 496.16765052080154\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 4 completed.\n",
      "Loss: 484.3252323269844\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 5 completed.\n",
      "Loss: 475.68764013051987\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 6 completed.\n",
      "Loss: 469.02505922317505\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 7 completed.\n",
      "Loss: 463.5093591809273\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 8 completed.\n",
      "Loss: 458.82539570331573\n",
      "==============================================\n",
      "==============================================\n",
      "Epoch 9 completed.\n",
      "Loss: 454.82355189323425\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(N_TRAINING_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/TRAINING_BATCH_SIZE)):\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(TRAINING_BATCH_SIZE)\n",
    "            \n",
    "            _, c = sess.run([optimizer, loss], feed_dict={\n",
    "                X: epoch_x,\n",
    "                y: epoch_y\n",
    "            })\n",
    "            epoch_loss += c\n",
    "            \n",
    "        print(\"==============================================\")\n",
    "        print(\"Epoch \" + str(epoch) + \" completed.\")\n",
    "        print(\"Loss: \" + str(epoch_loss))\n",
    "        print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example II: Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the text corpus\n",
    "text = open(\"./workshop-DL03/the-last-question.txt\",\"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text corpus into integers\n",
    "character_set = list(set(text))\n",
    "char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "n_characters = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 24 # length of char-sequences fed to RNN\n",
    "ELEMENT_LEN = 1 #n_characters # length of each element in a char-sequence\n",
    "\n",
    "N_CLASSES = 1 # n_characters\n",
    "\n",
    "N_TRAINING_EPOCHS = 20\n",
    "TRAINING_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_integers(text):\n",
    "    \"\"\"\n",
    "    Converts text into a list of integers.\n",
    "    \"\"\"\n",
    "    integers = []\n",
    "    for char in text:\n",
    "        integers.append([char_to_int[char]])\n",
    "    return integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "encoding = text_to_integers(text)\n",
    "train_X, train_y = [], []\n",
    "for i in range(0, len(encoding)-SEQUENCE_LEN, SEQUENCE_LEN):\n",
    "    train_X.append( encoding[i:i+SEQUENCE_LEN] )\n",
    "    train_y.append( encoding[i+SEQUENCE_LEN] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[1], train_y[1])\n",
    "print(train_X[2], train_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "        batches.append(data[i*batch_size:(i+1)*batch_size])\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_X = batchify(train_X, TRAINING_BATCH_SIZE)\n",
    "batches_y = batchify(train_y, TRAINING_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input placeholder / reshape\n",
    "with tf.variable_scope(\"input\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, SEQUENCE_LEN, ELEMENT_LEN])\n",
    "    X_transpose = tf.transpose(X, [1,0,2])\n",
    "    X_reshape = tf.reshape(X_transpose, [-1, ELEMENT_LEN])\n",
    "    X_split = tf.split(X_reshape, SEQUENCE_LEN, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network architecture\n",
    "with tf.variable_scope(\"rnn\"):\n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(128)\n",
    "    output, state = rnn.static_rnn(\n",
    "        lstm_cell, X_split, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output placeholder\n",
    "with tf.variable_scope(\"output\"):\n",
    "    W = tf.get_variable(\"weight\", shape=[128, N_CLASSES])\n",
    "    b = tf.get_variable(\"biases\", shape=[N_CLASSES])\n",
    "    y_pre_softmax = tf.matmul(output[-1], W) + b\n",
    "    #y_ = tf.nn.softmax(y_pre_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    y = tf.placeholder(tf.float32, shape=[None, N_CLASSES])\n",
    "    #entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pre_softmax, labels=y)\n",
    "    msqerr = (y - y_pre_softmax)**2\n",
    "    loss = tf.reduce_mean(msqerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # train the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1, N_TRAINING_EPOCHS+1):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for i in range(len(batches_X)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer],\n",
    "                feed_dict={\n",
    "                    X: batches_X[i],\n",
    "                    y: batches_y[i]\n",
    "                })\n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "            \n",
    "        print(\"==============================================\")\n",
    "        print(\"Epoch \" + str(epoch) + \" completed.\")\n",
    "        print(\"Loss: \" + str(epoch_loss))\n",
    "        print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
